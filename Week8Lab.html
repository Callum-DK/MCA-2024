<html>
<head>
  <title>My Portfolio Page 8</title>

</head>
<body>

  <h1>Week[8]</h1>
  
  <p>
    From the group lab session, we converted our chosen music peices to midi and played them through Musescore. 
    The music is played and represented as notes in a piano roll. The system takes into account pitch and rhythm
    as well as note length.
  </p>

  <p>
    It is noted that Musescore has its own midi instruments that are seamlessly looped and applied to the sheet 
    notation to playback.
  </p>

  <p>
    We exported our peice as a raw audio WAV file and imported this to sonic visualiser. An audio analysis app. 
    It was a stereo WAV file, primary data is located within the waveform. This waveform is drawn based on amplitude 
    and frequency. It shows the shape of a wave at a given time. This is a form of spectral representation.
  </p>

  <p>
    We created panes in sonic visualiser and generated a spectogram, all the percussive elements such as drums that
    have high amplitude could be seen visually as verticle peaks in the waveform. Pitch of a sound, was represented
    as parallel horizontal lines, where the lowest line corrosponds to the fundamental and the higher lines corrospond
    to the overtones. The spectogram shows the waveform as a heat map, with frequency on the vertical axis and time
    on the horizontal. Brightness of a colour on the heatmap determines the level of amplitude. Wave amplitude is 
    the distance between the peak or trough of a wave and its equilibrium position. Amplitude can be measured in decibles.
  </p>

<p>
  I discovered that it is possible to export the audio WAV file as a CSV file. Viewing this in an excel sheet, the table
  was filled with decible numbers. These act as specific time stamps within the peice. This could be used for fine audio editing.
  Can specifically be used for the likes of creating dubbed and synchronised versions of a video.
</p>

<p>
  Areas in space and time on spectrums within audio data will have a reference point listed as a numerical value within a CSV file.
</p>

  <p>
    Spectograms can help identify characterstics of signals that are non-stationary or nonlinear. We can deduce that audio data
    is represented differently from notation data. Audio data is represented in visual waveforms, graphs and heat maps. Notation
    is represented in print, with particualr rules. Notes in notation are seperated and put in sequencial order, they also factor
    in the type of instrument being used and the format can be change accordingly. A single note in raw audio data occupies multiple
    frequency bands. Audio data relates to original notation as we can see consistencies with notational rules and audio waveforms. 
    Note length, pitch and rhythm within sheet notation will respectfully corrospond to raw audio waveform analysis. Dynamics in sheet
    notation such as crescendo could relate to raw audio data and relate to the amplitude colour on a spectograph. Louder sounds
    will have a brighter colour due to more amplitude energy. 
  </p>

  <p>
    I identified 3 tracks in relation to my theme. My piece is a cinematic orchestral soundtrack
    for a film. I was able to download three cinematic style tracks that were different in sound and 
    style from the Free Music Archive. 
  </p>

  <p>
    <audio controls>
  <source src="Action Strings by Giacomo Forte.mp3" type="audio/mp3">
      </audio>
  </p>

  <p>
    <audio controls>
  <source src="Dreamworld by Denys Kyschchuk.mp3" type="audio/mp3">
      </audio>
    </p>

  <p>
     <audio controls>
  <source src="Hunt Him by Manuel Senfft.mp3" type="audio/mp3">
</audio>
  </p>

  <p>
    Sonic visualiser was used to gather some technical data from the tracks such as sample rate, bit depth, duration, 
    tempo and key. I thought these would be useful as regards my theme because Cinematic soundtracks being used for film
    will need to be high quality, and if they contain orchestral elements, chances are musicians will need notational information
    such as key and duration. 
  </p>

  <p>
  <img src="SonicRaw.png" alt="SonicRaw"
    style="max-width: 70%;"/>
    </p>

   <p>
  <img src="KeySonic.png" alt="KeySonic"
    style="max-width: 70%;"/>
    </p>


  <p>
To get the bits per second of each track I used the following bit rate calculation. Multiplying the bit depth by
  the sample rate. For example one track had 16 bits per sample and a sample rate of 44.1kHz. Converted kHz into Hz.
  </p>

  <p>
    The result was from 16x44,100 = 705,600 bps.
  </p>

  <p>
    The technical and non technical metadata was conducted into a table using google sheets. 
  </p>

  <p>
  <img src="Week8LabMetadata.png" alt="Week8LabMetadata"
    style="max-width: 70%;"/>
    </p>

  <p>
    I added new panes to each track in sonic visualiser to create a spectogram with a log spaced frequency scale.
  </p>

  <p>
    Action Strings by Gicomo Forte
  </p>

   <p>
  <img src="ImageSoniceActionStrings2.png" alt="ImageSonicActionStrings2"
    style="max-width: 100%;"/>
    </p>
  
<p>
  <img src="ImageSonicActionStrings1.png" alt="ImageSonicActionStrings1"
    style="max-width: 100%;"/>
    </p>
  
  <p>
    Dreamworld by Denys Kyschchuk
  </p>

  <p>
  <img src="ImageSonicDreamWorld1.png" alt="ImageSonicDreamWorld1"
    style="max-width: 100%;"/>
    </p>

    <p>
  <img src="ImageSonicDreamworld2.png" alt="ImageSonicDreamworld2"
    style="max-width: 100%;"/>
    </p>

   <p>
    Hunt Him by Manuel Senfft
  </p>

   <p>
  <img src="ImageSonicHuntHim1.png" alt="ImageSonicHuntHim1"
    style="max-width: 100%;"/>
    </p>

    <p>
  <img src="ImageSonicHuntHim2.png" alt="ImageSonicHuntHim2"
    style="max-width: 100%;"/>
    </p>

  <p>
    One advantage of time frequency analysis over waveform based analysis
    is that with time frequency you get simultaneous time and frequency information
    as opposed to waveform which provides amplitude information over time but does not
    inherently show frequency content. 
  </p>

  <p>
    Techniques like wavelet denoising leverage time-frequency representations to filter 
    out noise while maintaining the signal's essential characteristics, which might be 
    challenging with simple waveform analysis.
  </p>

  <p>
    Waveform analysis is capable of showing when an audio event takes place and give us information like
    amplitude and note length. However it doesn't tell us what frequency an event it occuring at
    unless we carry out further processing and analysis on the waveform. Time frequency analyisis can pinpoint
    when an event in audio occurs and at exactly what frequency and can tell us when and where the frequency changes.
  </p>

  <p>
    If we are analysing complex signals where many frequencies are at play at the same time. Waveform analysis
    may limit us in this regard. Time frequency analysis provides are more intuative analysis, giving us visual 
    representations through spectograms which are images with x axis for time and y axis for frequency or vice versa.
    We also get more information from time frequency analysis like colour intensity to represent changes in amplitude energy.
  </p>

  <p>
    As a final point, waveform Analysis assumes a signal is stationary, meaning its statistical properties do not change over time, 
    which is often not the case in real-world scenarios. Time-Frequency Analysis is specifically designed to deal with non-stationary 
    signals where frequency components evolve. This is particularly useful in fields like audio processing, seismic analysis, 
    or biomedical signal processing.
  </p>

  <p>
    To provide an example, I chose an exact point in the waveform analysis where the event of bass in the music kicks in.
    First off I was able to export the time frequency analysis as a data CSV file and open with a text editor to see the list of
    timestamp numerical data. Having positioned at my desired timestamp, I took the numerical value reference from sonic visualiser 
    as 266112. I located this timestamp within the CSV file and was able to log accruate coordinates that could be used for pinpointing
    an exact moment in the track. Additionally with the time frequency analysis we can analyse the frequencies present at this give moment. We
    can see the frequency values here at this moment are roughly 215. See visual evidence below. We can see how many frequencies are simultanously 
    layered and at what pitch. I have localised an audio event within the track and gained amplitude information from the waveform spectrum and 
    gathered this with the data gathered from the time frequency spectrum. 
  </p>

  <p>
    Here is some visual evidence:
  </p>

   <p>
  <img src="SonicCSV.png" alt="SonicCSV"
    style="max-width: 70%;"/>
    </p>
</body> 
</html>
